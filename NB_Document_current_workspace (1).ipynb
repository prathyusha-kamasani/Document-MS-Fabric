{"cells":[{"cell_type":"markdown","source":["## Document Current Workspace\n","Written by Prathy Kamasani\n","\n","https://prathy.com/\n","\n","https://www.linkedin.com/in/prathy/"],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"c1210c1e-28e7-4a8b-b1e9-a8b2a86d4951"},{"cell_type":"markdown","source":["## Clean Up"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81921e4a-4c4e-45ed-b380-8f9b3a18760f"},{"cell_type":"markdown","source":["If tables already exists, clean by dropping existing tables. Useful in development mode."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69a6d38f-91d0-4756-a94b-b6db2fc40395"},{"cell_type":"code","source":["# This is to delete all delta tables in LH\n","LH_Name = \"LH_Fabric_Documentation\"\n","\n","# Get all tables in the database\n","tables = spark.sql(f\"SHOW TABLES IN {LH_Name}\")\n","# display(tables)\n","\n","# Loop through the tables and drop each one\n","for table in tables.collect():\n","    table_name = table['tableName']\n","    spark.sql(f\"DROP TABLE {LH_Name}.{table_name}\")\n","\n","print(\"All Delta tables have been deleted.\")"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":true,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"2dd5a54a-fb8c-462b-b95c-a939e5977603"},{"cell_type":"markdown","source":["## Import all necessary items"],"metadata":{},"id":"bbb3c959"},{"cell_type":"code","source":["! pip install semantic-link\n","! pip install semantic-link-labs\n","! pip install delta-spark"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"77a387bb-fae1-4c4b-876b-bd477eef7e66"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","\n","import sempy_labs as sempy_labs\n","from sempy_labs import migration, report, directlake\n","from sempy_labs import lakehouse as lake\n","from sempy_labs.tom import connect_semantic_model\n","\n","from pyspark.sql.functions import current_timestamp\n","from pyspark.sql.functions import lit\n","\n","from delta.tables import DeltaTable\n","\n","import pandas as pd"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78fdb6b7-f6da-4b07-a3c5-1cd037001248"},{"cell_type":"markdown","source":["## Define items needed for the documentaion"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4cea4efe-a54e-42a2-8380-fcc72cf13290"},{"cell_type":"code","source":["# Define Lakehouse name and description. This will the LH where all documentation will be saved\n","LH_Name = \"LH_Fabric_Documentation\"\n","LH_desc = \"Lakehouse for Fabric Documentation\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6cfde015-8f46-4103-b069-0691bab6eed0"},{"cell_type":"code","source":["# Get current workspace details\n","current_workspace_id = fabric.get_workspace_id()\n","current_workspace_name = fabric.resolve_workspace_name(current_workspace_id)\n","print(f'Current workspace ID: {current_workspace_id}')\n","print(f'Current workspace name: {current_workspace_name}')\n","\n","# Check if the Lakehouse already exists, if not, create it\n","# List existing lakehouses and check if the specified one already exists\n","lakehouse_list = sempy_labs.list_lakehouses()\n","\n","if LH_Name in lakehouse_list['Lakehouse Name'].values:\n","    print(\"Lakehouse already exists\")\n","else:\n","    # Create a new Lakehouse\n","    mssparkutils.lakehouse.create(name=LH_Name, description=LH_desc, workspaceId=current_workspace_id)\n","    print(\"Lakehouse created successfully\")\n","\n","# Mount the Lakehouse for direct file system access\n","lakehouse = mssparkutils.lakehouse.get(LH_Name)\n","mssparkutils.fs.mount(lakehouse.get(\"properties\").get(\"abfsPath\"), f\"/{LH_Name}\")\n","\n","# Retrieve and store local and ABFS paths of the mounted Lakehouse\n","local_path = mssparkutils.fs.getMountPath(f\"/{LH_Name}\")\n","lh_abfs_path = lakehouse.get(\"properties\").get(\"abfsPath\")\n","print(f'Local path: {local_path}')\n","print(f'ABFS path: {lh_abfs_path}')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"cf606a5f-927e-45d3-89b1-f7d129c55007"},{"cell_type":"markdown","source":["#### Give list of Lakehouses or select the current Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"859e657d-0afe-41b6-a865-fa2e42dbd460"},{"cell_type":"markdown","source":["In this notebook, I am working to get one current workspace but idea is to be able to work with multiple workspaces"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d43343a7-b8f0-4141-af43-549c0aa02575"},{"cell_type":"code","source":["# Initialize the list of workspaces if it's empty populate with current workspace ID\n","list_of_workspaces = []\n","if not list_of_workspaces:\n","    list_of_workspaces.append(fabric.get_workspace_id())\n","print(\"Number of workspaces: \", len(list_of_workspaces))\n","print(list_of_workspaces)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c2f886f-4794-4101-94fa-2df426adf256"},{"cell_type":"markdown","source":["## Fabric Items"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1dd88329-79c0-4939-b2bd-49a7707234a6"},{"cell_type":"markdown","source":["To begin with, I am working with getting data for all fabric items of current workspace. If you don't give workspace name, then it will bring all tennant data based on your access. <mark>This could be slow if you are working on a big teannt. BE CAREFUL!</mark>\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"23aac8a2-2e78-48e1-b918-d248619ea43f"},{"cell_type":"code","source":["# Iterate through each workspace\n","for Current_workspace_name in list_of_workspaces:\n","    # Retrieve all items from the current fabric workspace\n","    Fabric_all_items = fabric.list_items(workspace=Current_workspace_name)\n","    \n","    # Transform column names for LH compatibility\n","    Fabric_all_items.columns = Fabric_all_items.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n","    Fabric_all_items.columns = Fabric_all_items.columns.str.replace('[ ]', '', regex=True)\n","    \n","    # Create a Spark DataFrame from the fabric items and save it to the Lakehouse\n","    sparkdf = spark.createDataFrame(Fabric_all_items)\n","    sparkdf = sparkdf.withColumn(\"LoadDate\", current_timestamp())\n","    Table_Name = \"R_fabric_all_items\"\n","    sparkdf.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{Table_Name}\")\n","    print(Table_Name, \"created at :\", f\"{lh_abfs_path}/Tables/{Table_Name}\")\n","    \n","    # Extract and save distinct item types from fabric items\n","    Fabric_all_items_type = Fabric_all_items['Type'].unique()\n","    \n","    # Create a Spark DataFrame for item types and save it to the Lakehouse\n","    spark_df_Fabric_all_items_type = spark.createDataFrame(Fabric_all_items_type)\n","    spark_df_Fabric_all_items_type = spark_df_Fabric_all_items_type.withColumnRenamed('value', 'Type')\n","    spark_df_Fabric_all_items_type = spark_df_Fabric_all_items_type.withColumn(\"LoadDate\", current_timestamp())\n","    Table_Name = \"R_fabric_item_types\"\n","    spark_df_Fabric_all_items_type.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{Table_Name}\")\n","    print(Table_Name, \"created at :\", f\"{lh_abfs_path}/Tables/{Table_Name}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3e9fac8-b3e2-43c7-85d7-b2b9364da215"},{"cell_type":"markdown","source":["### Create Presentation layer for Fabric Items  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3a7f0bd-e71a-4c2e-9bac-a52fc1cdf94a"},{"cell_type":"code","source":["fabric_all_items_path = f\"{lh_abfs_path}/Tables/R_fabric_all_items\"\n","fabric_item_types_path = f\"{lh_abfs_path}/Tables/R_fabric_item_types\"\n","distinct_fabric_all_items_path = f\"{lh_abfs_path}/Tables/Fabric_all_items\"\n","distinct_fabric_item_types_path = f\"{lh_abfs_path}/Tables/Fabric_item_types\"\n","\n","# Get distinct fabric all items\n","df_fabric_all_items = spark.read.parquet(fabric_all_items_path)\n","df_fabric_all_items = df_fabric_all_items.drop(\"LoadDate\").dropDuplicates()\n","df_fabric_all_items.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(distinct_fabric_all_items_path)\n","\n","# Get distinct fabric item types\n","df_fabric_item_types = spark.read.parquet(fabric_item_types_path)\n","df_fabric_item_types = df_fabric_item_types.drop(\"LoadDate\").dropDuplicates()\n","df_fabric_item_types.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(distinct_fabric_item_types_path)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd7a44a9-a56c-443c-9fe3-7d586928c925"},{"cell_type":"markdown","source":["## Lakehouses"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67dd6f27-8460-47bf-be54-aa43c24c4be9"},{"cell_type":"code","source":["# Filter for Lakehouse items\n","fabric_all_lakehouses = Fabric_all_items[Fabric_all_items['Type'] == 'Lakehouse']\n","fabric_all_lakehouses\n","\n","# e.g. Get Lakehouse tables\n","lakehouse_name = \"ReleasePlan\"\n","lakehouse_tables = sempy_labs.lakehouse.get_lakehouse_tables(lakehouse_name, current_workspace_name)\n","display(lakehouse_tables)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"2b7a0325-4574-44db-9c4f-fb7dc5ea4d17"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Initialize an empty DataFrame to store all tables\n","all_tables_df = pd.DataFrame()\n","\n","# Loop through each lakehouse and get its tables\n","for lakehouse_name in fabric_all_lakehouses['DisplayName']:\n","    lakehouse_tables = sempy_labs.lakehouse.get_lakehouse_tables(lakehouse_name, current_workspace_name)\n","    \n","    # Convert the tables to a DataFrame\n","    lakehouse_tables_df = pd.DataFrame(lakehouse_tables)\n","    \n","    # Append to the main DataFrame\n","    all_tables_df = pd.concat([all_tables_df, lakehouse_tables_df], ignore_index=True)\n","\n","# Transform column names for LH compatibility\n","all_tables_df.columns = all_tables_df.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n","all_tables_df.columns = all_tables_df.columns.str.replace('[ ]', '', regex=True)\n","# Display the combined DataFrame\n","display(all_tables_df)\n","\n","sdf_df_LHTables= spark.createDataFrame(all_tables_df)\n","\n","table_name = \"R_Lakehouse_Tables\"\n","sdf_df_LHTables.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{table_name}\")\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"38ae07ed-c89f-49c7-aa46-84d0d589c6d7"},{"cell_type":"markdown","source":["Selecting only subset of lakehouses to make it easy"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"570f5587-ef2d-42fe-b1fd-9ad799d89ea6"},{"cell_type":"code","source":["# Group by workspace and lakehouse name, then count the number of tables\n","grouped_df = all_tables_df.groupby(['WorkspaceName', 'LakehouseName']).size().reset_index(name='Number_of_Tables')\n","\n","# Filter to work with a small set of data\n","filtered_LH_df = grouped_df[grouped_df['Number_of_Tables'] <= 5]\n","\n","# Display the filtered DataFrame\n","filtered_LH_df"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"924145fa-3c38-4807-b417-b9f478f622e2"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","import pandas as pd\n","import json\n","\n","# Assuming Fabric_all_lakehouses is a DataFrame with lakehouse details\n","for _, lakehouse_row in filtered_LH_df.iterrows():\n","    lakehouse_name = lakehouse_row['LakehouseName']\n","    Workspace_name = lakehouse_row['WorkspaceName']\n","    \n","    try:\n","        lakehouse_tables = sempy_labs.lakehouse.get_lakehouse_tables(lakehouse_name, Workspace_name)\n","    except Exception as e:\n","        print(f\"Error fetching tables for lakehouse {lakehouse_name}: {e}\")\n","        continue\n","    \n","    for _, table_row in lakehouse_tables.iterrows():\n","        path = table_row['Location']\n","        LH_table_name = table_row['Table Name']\n","        \n","        try:\n","            delta_table = DeltaTable.forPath(spark, path)\n","            detail_df = delta_table.detail().toPandas()\n","        except Exception as e:\n","            print(f\"Error processing table {LH_table_name} at {path}: {e}\")\n","            continue\n","\n","        delta_table = DeltaTable.forPath(spark, path)\n","        detail_df = delta_table.detail().toPandas()\n","        # display(detail_df)\n","        detail_df = detail_df.astype(str)\n","\n","        # Add new columns to the DataFrame\n","        detail_df['Lakehouse_name'] = lakehouse_name\n","        detail_df['Table_Name'] = LH_table_name\n","        \n","        try:\n","            spark_pandas_df = spark.createDataFrame(detail_df)\n","            spark_pandas_df = spark_pandas_df.withColumn(\"LoadDate\", current_timestamp())\n","            # spark_pandas_df.show()\n","            table_name = \"R_Lakehouse_Tables_Details\"\n","            spark_pandas_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{table_name}\")\n","            print(table_name, \"created at:\", f\"{lh_abfs_path}/Tables/{table_name}\",\"for LH table - \",{LH_table_name})\n","        except Exception as e:\n","            print(f\"Error saving table {table_name}: {e}\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"4c5f2576-ee3b-4a60-a10d-69394ec0c473"},{"cell_type":"markdown","source":["Lakehouse Presentation Layer"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"081f4c66-3893-4816-ba8f-d2d010459c0b"},{"cell_type":"code","source":["R_LakehouseTables_path = f\"{lh_abfs_path}/Tables/R_Lakehouse_Tables\"\n","Distinct_LakehouseTables_path = f\"{lh_abfs_path}/Tables/Lakehouse_Tables\"\n","\n","# Get distinct LH tables\n","df_LH_Tables = spark.read.parquet(R_LakehouseTables_path)\n","df_LH_Tables = df_LH_Tables.drop(\"LoadDate\").dropDuplicates()\n","df_LH_Tables.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(Distinct_LakehouseTables_path)\n","\n","\n","R_LakehouseTablesDetails_path = f\"{lh_abfs_path}/Tables/R_Lakehouse_Tables_Details\"\n","Distinct_LakehouseTablesDetails_path = f\"{lh_abfs_path}/Tables/Lakehouse_Tables_Details\"\n","\n","\n","# Get distinct LH tables\n","df_LH_Tables = spark.read.parquet(R_LakehouseTablesDetails_path)\n","df_LH_Tables = df_LH_Tables.drop(\"LoadDate\").dropDuplicates()\n","df_LH_Tables.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(Distinct_LakehouseTablesDetails_path)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76ef72a9-ce96-4312-ad2f-3d0ff125cae8"},{"cell_type":"markdown","source":["## Semantic models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a63f9537-f6b7-4369-bc88-f7eae00a4426"},{"cell_type":"code","source":["from delta.tables import DeltaTable\n","import pandas as pd\n","from pyspark.sql.functions import lit, current_timestamp\n","\n","# Initialize an empty list to store semantic model objects\n","list_semantic_model_objects = []\n","\n","# Filter the DataFrame to include only items of type 'SemanticModel'\n","df_semantic_models = Fabric_all_items[(Fabric_all_items['Type'] == 'SemanticModel') & (Fabric_all_items['DisplayName'] != 'Report Usage Metrics Model')]\n","sdf_df_semantic_models = spark.createDataFrame(df_semantic_models)\n","table_name = \"R_semantic_models\"\n","sdf_df_semantic_models.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{table_name}\")\n","\n","# Iterate through each semantic model, retrieve its objects, and append to the list\n","for _, row in df_semantic_models.iterrows():\n","    dataset_name = row['DisplayName']\n","    try:\n","        semantic_model_objects = sempy_labs.list_semantic_model_objects(dataset_name, current_workspace_name)\n","    except Exception as e:\n","        print(f\"Error fetching semantic model objects for {dataset_name}: {e}\")\n","        continue\n","\n","    if not semantic_model_objects.empty:\n","        df_semantic_model_objects = pd.DataFrame(semantic_model_objects)\n","        df_semantic_model_objects.columns = df_semantic_model_objects.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n","        df_semantic_model_objects.columns = df_semantic_model_objects.columns.str.replace(' ', '', regex=True)\n","        \n","        spark_df_semantic_models = spark.createDataFrame(df_semantic_model_objects)\n","        spark_df_semantic_models = spark_df_semantic_models.withColumn(\"LoadDate\", current_timestamp())\n","        spark_df_semantic_models = spark_df_semantic_models.withColumn(\"WorkspaceName\", lit(current_workspace_name))\n","        spark_df_semantic_models = spark_df_semantic_models.withColumn(\"DatasetName\", lit(dataset_name))\n","        \n","        table_name = \"R_semantic_model_objects\"\n","        spark_df_semantic_models.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{table_name}\")\n","        print(f\"{table_name} created/updated at: {lh_abfs_path}/Tables/{table_name} for {dataset_name}\")\n","\n","# Extract and save distinct item types from fabric items\n","df_semantic_model_objects_type = df_semantic_model_objects['ObjectType'].unique()\n","\n","# Create a Spark DataFrame for item types and save it to the Lakehouse\n","spark_df_semantic_model_objects_type = spark.createDataFrame(df_semantic_model_objects_type)\n","spark_df_semantic_model_objects_type = spark_df_semantic_model_objects_type.withColumn(\"LoadDate\", current_timestamp())\n","table_name = \"R_semantic_model_object_types\"\n","spark_df_semantic_model_objects_type.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{table_name}\")\n","print(f\"{table_name} created at: {lh_abfs_path}/Tables/{table_name}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa65a44c-0853-4351-bcc3-de06f19f0734"},{"cell_type":"markdown","source":["### **Create data for Semnatic models and create presentation Layer**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"696e700f-452e-4a3e-b724-ef01caeefc93"},{"cell_type":"code","source":["# Get distinct semantic model object types\n","df_semantic_model_object_types = spark.read.parquet(f\"{lh_abfs_path}/Tables/R_semantic_model_object_types\")\n","df_semantic_model_object_types = df_semantic_model_object_types.drop(\"LoadDate\").dropDuplicates()\n","\n","# Rename column 'value' to 'ObjectType'\n","df_semantic_model_object_types = df_semantic_model_object_types.withColumnRenamed('value', 'ObjectType')\n","df_semantic_model_object_types.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"{lh_abfs_path}/Tables/semantic_model_object_types\")\n","\n","# Get distinct object types from semantic model objects\n","df_object_types = spark.read.parquet(f\"{lh_abfs_path}/Tables/R_semantic_model_objects\")\n","df_object_types = df_object_types.drop(\"LoadDate\").dropDuplicates()\n","df_object_types.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"{lh_abfs_path}/Tables/semantic_model_objects\")\n","\n","# Get distinct semantic models\n","df_semantic_models = spark.read.parquet(f\"{lh_abfs_path}/Tables/R_semantic_models\")\n","df_semantic_models = df_semantic_models.drop(\"LoadDate\").dropDuplicates()\n","\n","# Rename 'DisplayName' to 'DatasetName' and select relevant columns\n","df_semantic_models = df_semantic_models.withColumnRenamed('DisplayName', 'DatasetName').select('DatasetName', 'Id', 'WorkspaceID', 'Description')\n","df_semantic_models.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"{lh_abfs_path}/Tables/semantic_models\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75818f00-8c8d-4a42-ab64-90a51199671d"},{"cell_type":"markdown","source":["### Semantic models and underline data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"712a7749-60ef-439b-b2ce-c6b75f4453b5"},{"cell_type":"code","source":["# from pyspark.sql.functions import col, lit\n","\n","# # Read the parquet file\n","# df = spark.read.parquet(f\"{lh_abfs_path}/Tables/R_semantic_model_objects\")\n","\n","# # Select all columns except 'LoadDate' and remove duplicates\n","# df = df.drop(\"LoadDate\").dropDuplicates()\n","\n","# # Get distinct object types\n","# object_types = df.select('ObjectType').distinct().rdd.flatMap(lambda x: x).collect()\n","\n","# # Function to process each object type\n","# def process_object_type(object_type):\n","#     df_object_type = df.filter(col('ObjectType') == object_type)\n","    \n","#     # Rename 'ObjectName' to 'ObjectType_Name' and drop 'ObjectType' column\n","#     df_object_type = df_object_type.withColumnRenamed('ObjectName', f'{object_type}_Name').drop('ObjectType')\n","    \n","#     # Remove special characters from column names\n","#     for column in df_object_type.columns:\n","#         new_column = column.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '')\n","#         df_object_type = df_object_type.withColumnRenamed(column, new_column)\n","    \n","#     # Replace spaces with underscores in the object type for the filename\n","#     object_type_filename = object_type.replace(' ', '_')\n","    \n","#     # Save the DataFrame as a Delta table\n","#     df_object_type.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{lh_abfs_path}/Tables/{object_type_filename}\")\n","\n","# # Loop through each object type and process the data\n","# for object_type in object_types:\n","#     process_object_type(object_type)\n","\n","# # Combine Table and Calculated_Table to create a single table with a 'Type' column to differentiate between the two\n","# df_tables = spark.read.parquet(f\"{lh_abfs_path}/Tables/Table\")\n","# df_calculated_tables = spark.read.parquet(f\"{lh_abfs_path}/Tables/Calculated_Table\")\n","\n","# # Rename column 'Calculated_Table' to 'Table_Name'\n","# df_calculated_tables = df_calculated_tables.withColumnRenamed('Calculated_Table', 'Table_Name')\n","\n","# # Add 'Type' column to each DataFrame\n","# df_tables = df_tables.withColumn('Type', lit('Table'))\n","# df_calculated_tables = df_calculated_tables.withColumn('Type', lit('Calculated Table'))\n","\n","# # Combine the DataFrames and remove duplicates\n","# df_combined_tables = df_tables.union(df_calculated_tables).drop(\"LoadDate\").dropDuplicates()\n","\n","# # Save the combined DataFrame as a Delta table\n","# df_combined_tables.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"{lh_abfs_path}/Tables/SM_Tables\")\n","\n","# # Get distinct columns and save as a Delta table\n","# df_columns = spark.read.parquet(f\"{lh_abfs_path}/Tables/Column\")\n","# df_columns.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"overwrite\").save(f\"{lh_abfs_path}/Tables/SM_Columns\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb1b64b2-8628-4cc4-9fb8-d20140f300b6"},{"cell_type":"markdown","source":["## **Vertipaq Analyser for Semantic Models data**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0f15a35-9c9c-4480-a1ff-408f2c76eaf8"},{"cell_type":"code","source":["df_semantic_models = Fabric_all_items[(Fabric_all_items['Type'] == 'SemanticModel') & (Fabric_all_items['DisplayName'] != 'Report Usage Metrics Model')]\n","\n","# Initialize an empty list to store semantic model objects\n","list_semantic_model_objects_vertipaq_analyzer = []\n","\n","# Iterate through each semantic model, retrieve its objects, and append to the list\n","for index, row in df_semantic_models.iterrows():\n","    try:\n","        Dataset_Name = row['DisplayName']\n","        print(f'Dataset_Name - {Dataset_Name}')\n","        # sempy_labs.vertipaq_analyzer(dataset=Dataset_Name)\n","        sempy_labs.vertipaq_analyzer(dataset = Dataset_Name,workspace=Current_workspace_name, export = 'table')\n","        # sempy_labs.vertipaq_analyzer(dataset=Dataset_Name, workspace=Current_workspace_name)\n","    except KeyError as e:\n","        print(f\"KeyError: {e} - Check if 'DisplayName' exists in the DataFrame.\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","\n","print(\"Processing complete.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05882c7a-4fab-4ff5-8d0b-c8a036c3fbae"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"e233d883-22f9-4b75-bac6-3b57a0ff35b6","default_lakehouse_name":"LH_Fabric_Documentation","default_lakehouse_workspace_id":"41706d46-7c89-45c1-b56f-74aca7d99000"}}},"nbformat":4,"nbformat_minor":5}